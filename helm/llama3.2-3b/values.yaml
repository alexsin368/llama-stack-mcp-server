# Default values for llama3.2-3b
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  create: true
  name: ""

# Hardware configuration
hardware: "hpu"  # Options: "gpu", "cpu", "hpu"

# Model configuration
model:
  maxModelLen: 8192

# Resource requirements (adjust based on availability)
resources:
  gpu:
    limits:
      nvidia.com/gpu: 1
      memory: 24Gi
      cpu: 4
    requests:
      nvidia.com/gpu: 1
      memory: 16Gi
      cpu: 2
  cpu:
    limits:
      cpu: '8'
      memory: 24Gi
    requests:
      cpu: '4'
      memory: 12Gi
  hpu:
    limits:
      cpu: '6'
      memory: 16Gi
      habana.ai/gaudi: '1'
    requests:
      cpu: '4'
      memory: 8Gi
      habana.ai/gaudi: '1'

nodeSelector:
  gpu:
    nvidia.com/gpu.present: "true"
  cpu: {}
  hpu:
    habana.ai/gaudi: "true"

tolerations:
  gpu:
    - effect: NoSchedule
      key: nvidia.com/gpu
      value: NVIDIA-A10G-SHARED
  cpu: []
  hpu:
    - key: "habana.ai/gaudi"
      operator: "Exists"
      effect: "NoSchedule"

affinity:
  gpu:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: nvidia.com/gpu.present
            operator: In
            values:
            - "true"
  cpu: {}
  hpu: {}

# Environment variables
env:
  CUDA_VISIBLE_DEVICES: "0"
  TRANSFORMERS_CACHE: "/root/.cache/huggingface"
  HF_HOME: "/root/.cache/huggingface"

# modelHardware configuration, used for both inferenceService and servingRuntime
modelHardware:
  gpu: 
    name: "llama3-2-3b-gpu"
    displayName: "llama3.2-3b-gpu"
  cpu: 
    name: "llama3-2-3b-cpu"
    displayName: "llama3.2-3b-cpu"
  hpu: 
    name: "llama3-2-3b-hpu"
    displayName: "llama3.2-3b-hpu"

# InferenceService configuration
inferenceService:
  common:
    maxReplicas: 1
    minReplicas: 1
    modelFormat: "vLLM"
    modelName: ""
    storageUri: "oci://quay.io/redhat-ai-services/modelcar-catalog:llama-3.2-3b-instruct"

# Serving Runtime configuration
servingRuntime:
  common:
    enabled: true  # Set to true to create a ServingRuntime resource
    shmSizeLimit: "1Gi"
    memBufferBytes: 134217728  # 128MB
    modelLoadingTimeoutMillis: 90000  # 90 seconds
  gpu:
    recommendedAccelerators:
      - "nvidia.com/gpu"
    templateDisplayName: "vLLM ServingRuntime for KServe"
    templateName: "vllm-runtime"
    acceleratorName: "gpu"
    image: "quay.io/modh/vllm@sha256:0d55419f3d168fd80868a36ac89815dded9e063937a8409b7edf3529771383f3"
  cpu:
    recommendedAccelerators:
      - "cpu"
    templateDisplayName: "vLLM CPU ServingRuntime for KServe"
    templateName: "vllm-cpu-runtime"
    acceleratorName: "None"
    image: "public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo:latest"
  hpu:
    recommendedAccelerators:
      - "habana.ai/gaudi"
    templateDisplayName: "vLLM Intel Gaudi Accelerator ServingRuntime for KServe"
    templateName: "vllm-gaudi-runtime"
    acceleratorName: "gaudi-hpu-profile"
    image: "quay.io/modh/vllm@sha256:af6a071be36d8a99476f145d1589d7ede97d2760b93335b14ca26de7417e438c"