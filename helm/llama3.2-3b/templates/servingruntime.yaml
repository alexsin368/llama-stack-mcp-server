{{- if .Values.servingRuntime.common.enabled }}
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: {{ index .Values.modelHardware .Values.hardware "name" | default "llama3-2-3b" }}
  labels:
    {{- include "llama3-2-3b.labels" . | nindent 4 }}
  annotations:
    opendatahub.io/apiProtocol: REST
    opendatahub.io/recommended-accelerators: {{ index .Values.servingRuntime .Values.hardware "recommendedAccelerators" | default (list "nvidia.com/gpu") | join "," }}
    opendatahub.io/template-display-name: {{ index .Values.servingRuntime .Values.hardware "templateDisplayName" | default "vLLM ServingRuntime for KServe" }}
    opendatahub.io/template-name: {{ index .Values.servingRuntime .Values.hardware "templateName" | default "vllm_runtime" }}
    opendatahub.io/accelerator-name: {{ index .Values.servingRuntime .Values.hardware "acceleratorName" | default "migrated-gpu" }}
    openshift.io/display-name: {{ index .Values.modelHardware .Values.hardware "displayName" | default "llama3-2-3b" }}
spec:
  supportedModelFormats:
  - name: vLLM
    version: "1"
    autoSelect: true
  - name: huggingface
    version: "1"
    autoSelect: true
  containers:
  - name: kserve-container
    image: {{ index .Values.servingRuntime .Values.hardware "image" | default "quay.io/modh/vllm@sha256:0d55419f3d168fd80868a36ac89815dded9e063937a8409b7edf3529771383f3" }}
    args:
    - --model=/mnt/models
    - --port=8080
    - --max-model-len={{ .Values.model.maxModelLen | default 8192 }}
    - '--served-model-name=llama3-2-3b'
    - --chat-template=/app/data/template/tool_chat_template_llama3.2_json.jinja
    - '--enable-auto-tool-choice'
    - '--tool-call-parser'
    env:
    - name: HF_HOME
      value: /tmp/hf_home
    {{- range $key, $value := .Values.env }}
    {{- if ne $key "HF_HOME" }}
    - name: {{ $key }}
      value: {{ $value | quote }}
    {{- end }}
    {{- end }}
    ports:
    - containerPort: 8080
      name: h2c
      protocol: TCP
    {{- with (index .Values.resources .Values.hardware) }}
    resources:
      {{- toYaml . | nindent 6 }}
    {{- end }}
    volumeMounts:
    - name: shm
      mountPath: /dev/shm
    {{- if eq .Values.hardware "cpu" }}
    - name: chat-template-volume
      mountPath: /app/data/template/tool_chat_template_llama3.2_json.jinja
      subPath: tool_chat_template_llama3.2_json.jinja
    {{- end }}
  volumes:
  - name: shm
    emptyDir:
      medium: Memory
      sizeLimit: {{ .Values.servingRuntime.common.shmSizeLimit | default "1Gi" }}
  {{- if eq .Values.hardware "cpu" }}
  - name: chat-template-volume
    configMap:
      name: chat-template
  {{- end }}
  {{- with (index .Values.nodeSelector .Values.hardware) }}
  nodeSelector:
    {{- toYaml . | nindent 4 }}
  {{- end }}
  {{- with (index .Values.affinity .Values.hardware) }}
  affinity:
    {{- toYaml . | nindent 4 }}
  {{- end }}
  {{- with (index .Values.tolerations .Values.hardware) }}
  tolerations:
    {{- toYaml . | nindent 4 }}
  {{- end }}
  builtInAdapter:
    serverType: vllm
    runtimeManagementPort: 8080
    memBufferBytes: {{ .Values.servingRuntime.common.memBufferBytes | default 134217728 }}
    modelLoadingTimeoutMillis: {{ .Values.servingRuntime.common.modelLoadingTimeoutMillis | default 90000 }}
{{- end }}
