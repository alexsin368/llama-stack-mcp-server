{{- if .Values.servingRuntime.enabled }}
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: {{ .Values.servingRuntime.name | default "llama32-3b" }}
  labels:
    {{- include "llama3-2-3b.labels" . | nindent 4 }}
  annotations:
    opendatahub.io/apiProtocol: REST
    opendatahub.io/recommended-accelerators: {{ .Values.servingRuntime.recommendedAccelerators | join "," | default "nvidia.com/gpu" }}
    opendatahub.io/template-display-name: {{ .Values.servingRuntime.templateDisplayName | default "vLLM ServingRuntime for KServe" }}
    opendatahub.io/template-name: {{ .Values.servingRuntime.templateName | default "vllm_runtime" }}
    opendatahub.io/accelerator-name: {{ .Values.servingRuntime.acceleratorName | default "migrated-gpu" }}
    openshift.io/display-name: {{ .Values.servingRuntime.displayName | default "llama32-3b" }}
spec:
  supportedModelFormats:
  - name: vLLM
    version: "1"
    autoSelect: true
  - name: huggingface
    version: "1"
    autoSelect: true
  containers:
  - name: kserve-container
    image: {{ .Values.servingRuntime.image | default "quay.io/modh/vllm@sha256:0d55419f3d168fd80868a36ac89815dded9e063937a8409b7edf3529771383f3" }}
    args:
    - --model=/mnt/models
    - --port=8080
    - --max-model-len={{ .Values.model.maxModelLen | default 8192 }}
    - '--served-model-name=llama3-2-3b'
    - --chat-template=/app/data/template/tool_chat_template_llama3.2_json.jinja
    - '--enable-auto-tool-choice'
    - '--tool-call-parser'
    - llama3_json
    env:
    - name: HF_HOME
      value: /tmp/hf_home
    {{- range $key, $value := .Values.env }}
    {{- if ne $key "HF_HOME" }}
    - name: {{ $key }}
      value: {{ $value | quote }}
    {{- end }}
    {{- end }}
    ports:
    - containerPort: 8080
      name: h2c
      protocol: TCP
    resources:
      {{- if eq .Values.hardware "gpu" }}
      {{- toYaml .Values.resources.gpu | nindent 6 }}
      {{- else if eq .Values.hardware "cpu" }}
      {{- toYaml .Values.resources.cpu | nindent 6 }}
      {{- else if eq .Values.hardware "hpu" }}
      {{- toYaml .Values.resources.hpu | nindent 6 }}
      {{- end }}
    volumeMounts:
    - name: shm
      mountPath: /dev/shm
  volumes:
  - name: shm
    emptyDir:
      medium: Memory
      sizeLimit: {{ .Values.servingRuntime.shmSizeLimit | default "1Gi" }}
  {{- with .Values.nodeSelector }}
  nodeSelector:
    {{- if eq .Values.hardware "gpu" }}
    {{- toYaml .Values.nodeSelector.gpu | nindent 4 }}
    {{- else if eq .Values.hardware "cpu" }}
    {{- toYaml .Values.nodeSelector.cpu | nindent 4 }}
    {{- else if eq .Values.hardware "hpu" }}
    {{- toYaml .Values.nodeSelector.hpu | nindent 4 }}
    {{- end }}
  {{- end }}
  {{- with .Values.affinity }}
  affinity:
    {{- toYaml . | nindent 4 }}
  {{- end }}
  {{- with .Values.tolerations }}
  tolerations:
    {{- if eq .Values.hardware "gpu" }}
    {{- toYaml .Values.tolerations.gpu | nindent 4 }}
    {{- else if eq .Values.hardware "cpu" }}
    {{- toYaml .Values.tolerations.cpu | nindent 4 }}
    {{- else if eq .Values.hardware "hpu" }}
    {{- toYaml .Values.tolerations.hpu | nindent 4 }}
    {{- end }}
  {{- end }}
  builtInAdapter:
    serverType: vllm
    runtimeManagementPort: 8080
    memBufferBytes: {{ .Values.servingRuntime.memBufferBytes | default 134217728 }}
    modelLoadingTimeoutMillis: {{ .Values.servingRuntime.modelLoadingTimeoutMillis | default 90000 }}
{{- end }}